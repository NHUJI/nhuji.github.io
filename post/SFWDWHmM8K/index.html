<!DOCTYPE html>
<html>
  <head>
    <!-- 代码高亮 -->
    <link rel="stylesheet" type="text/css" href="https://nhuji.github.io/highlight/styles/github-dark.css">
    <script src="https://nhuji.github.io/highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <meta charset="utf-8" >
<meta name="msvalidate.01" content="A81BF2369C00030213C4032E982E497F" />

<title>No.1 ChatGPT 的训练数据来源 | Huhu&#39;s blog</title>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DW92LC8QYB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DW92LC8QYB');
</script>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<script src="https://kit.fontawesome.com/e8bf4d3f65.js" crossorigin="anonymous"></script>
<link rel="shortcut icon" href="https://nhuji.github.io/favicon.ico?v=1753943723267">
<link rel="stylesheet" href="https://nhuji.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="LLM 究竟是什么呢? 一方面，它确实有一些非常神奇、令人惊叹的能力；另一方面，它在某些方面也并不擅长。所以, 在这个对话框背后究竟是什么？我们输入任何东西，按下回车，会出现一段文字——产生这些文字的原理是什么？我们到底在和什么“对话”？ ..." />
    <meta name="keywords" content="LLM" />
    
<!-- doodle彩蛋按钮 -->
    <style>
      button,
      button::after {
       padding: 16px 20px;
       font-size: 18px;
       background: linear-gradient(45deg, transparent 5%, #ff013c 5%);
       border: 0;
       color: #fff;
       letter-spacing: 3px;
       line-height: 1;
       box-shadow: 6px 0px 0px #00e6f6;
       outline: transparent;
       position: relative;
       /*display: flex;
       justify-content: center;
       align-items: center;*/
      }

      button::after {
       --slice-0: inset(50% 50% 50% 50%);
       --slice-1: inset(80% -6px 0 0);
       --slice-2: inset(50% -6px 30% 0);
       --slice-3: inset(10% -6px 85% 0);
       --slice-4: inset(40% -6px 43% 0);
       --slice-5: inset(80% -6px 5% 0);
       content: "HOVER ME";
       display: block;
       position: absolute;
       top: 0;
       left: 0;
       right: 0;
       bottom: 0;
       background: linear-gradient(45deg, transparent 3%, #00e6f6 3%, #00e6f6 5%, #ff013c 5%);
       text-shadow: -3px -3px 0px #f8f005, 3px 3px 0px #00e6f6;
       clip-path: var(--slice-0);
      }

      button:hover::after {
       animation: 1s glitch;
       animation-timing-function: steps(2, end);
      }

      @keyframes glitch {
       0% {
        clip-path: var(--slice-1);
        transform: translate(-20px, -10px);
       }

       10% {
        clip-path: var(--slice-3);
        transform: translate(10px, 10px);
       }

       20% {
        clip-path: var(--slice-1);
        transform: translate(-10px, 10px);
       }

       30% {
        clip-path: var(--slice-3);
        transform: translate(0px, 5px);
       }

       40% {
        clip-path: var(--slice-2);
        transform: translate(-5px, 0px);
       }

       50% {
        clip-path: var(--slice-3);
        transform: translate(5px, 0px);
       }

       60% {
        clip-path: var(--slice-4);
        transform: translate(5px, 10px);
       }

       70% {
        clip-path: var(--slice-2);
        transform: translate(-10px, 10px);
       }

       80% {
        clip-path: var(--slice-5);
        transform: translate(20px, -10px);
       }

       90% {
        clip-path: var(--slice-1);
        transform: translate(-10px, 0px);
       }

       100% {
        clip-path: var(--slice-1);
        transform: translate(0);
       }
      }

    </style>


  </head>

  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://nhuji.github.io">
      <!-- 头像 
        <img src="https://nhuji.github.io/images/avatar.png?v=1753943723267" class="site-logo">
      -->
        <div class="site-logo">
          <img src="https://nhuji.github.io/images/avatar2.png?v=1753943723267" class="site-logo-image-back">
          <img src="https://nhuji.github.io/images/avatar.png?v=1753943723267" class="site-logo-image">
        </div>
        <h1 class="site-title">Huhu&#39;s blog</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            目录
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/NHUJI" target="_blank">
            <i class="fa-brands fa-github"></i>
          </a>
        
      
        
      
        
      

      <a class="social-link" href="mailto:hujinfinite@gmail.com" target="_blank">
      <i class="fa-regular fa-envelope"></i>
      </a>

    </div>
    <div class="site-description">
      It's me, huhu
    </div>
    <div class="site-footer">
      <a href="https://github.com/NHUJI" target="_blank">© 2021~2025 Nhuji</a> | <a class="rss" href="https://nhuji.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">No.1 ChatGPT 的训练数据来源</h2>
            <div class="post-date">2025-03-16</div>
            
            <div class="post-content" v-pre>
              <p>LLM 究竟是什么呢? 一方面，它确实有一些非常神奇、令人惊叹的能力；另一方面，它在某些方面也并不擅长。所以, 在这个对话框背后究竟是什么？我们输入任何东西，按下回车，会出现一段文字——产生这些文字的原理是什么？我们到底在和什么“对话”？ 相信如果我们能大概了解它的能力,也对我们更好地使用它有很大的帮助(而不是说我们真的要去训练模型)</p>
<!-- more -->
<p>接下来的几篇文档里, 将会介绍一个典型的 LLM 的训练过程 (主要参考了 Andrej Karpathy 的介绍)</p>
<h3 id="数据集">数据集</h3>
<p>所以类似 ChatGPT, Claude 这样的东西，是怎么“造”出来的？</p>
<p>要构造 ChatGPT 这类模型，一般会经历多个阶段，依次串行处理。第一个阶段叫做“预训练”（pre-training）。在预训练阶段开始时，第一个步骤就是：<strong>下载并处理互联网数据</strong>。直观地说，模型背后的公司需要把互联网的文本内容抓取下来，进行整理过滤，让模型进行“海量阅读”。</p>
<p>比如 Hugging Face 公司的 “<a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">Fine Web</a>” 数据集, 就会把互联网内容精简到最终 44TB 左右的文本数据</p>
<p><img src="https://nhuji.github.io/post-images/1753857553676.png" alt="" loading="lazy"><br>
像是 OpenAI、Anthropic、Google 等在训练时也会有类似的做法——从互联网上收集大规模、高质量且多样化的文本，并对其进行层层过滤。例如要保证文本足够多元化，又要保证文本质量，还要剔除大量无意义的垃圾内容。</p>
<h3 id="常见的互联网数据来源common-crawl">常见的互联网数据来源：Common Crawl</h3>
<p>很多公司会从 Common Crawl 拿数据。Common Crawl 是一个公益组织，从 2007 年开始就在全网爬取网页，截止 2024 年已经爬取了 27 亿个网页。它们会不断遍历互联网的超链接，获取各种网站的数据存档，并保存这些网页（主要是 HTML 页面）。但这些数据是非常原始的，所以在拿来使用前会有很多后续加工。比如下面这些步骤：</p>
<p><img src="https://nhuji.github.io/post-images/1753857567584.png" alt="" loading="lazy"><br>
(Claude 帮忙画的图,还不错吧)</p>
<ul>
<li><strong>URL 过滤</strong>：一些恶意网站或不良内容的网站（比如恶意软件、垃圾营销、极端内容、成人站点等）会在这一环节直接排除。(使用比如 https://dsi.ut-capitole.fr/blacklists/  这样的列表)</li>
<li><strong>文本抽取</strong>：HTML 网页有各种标签、CSS、JS，我们只想要其中的正文文本，所以要用一些算法/启发式方法尽量把网页上的“正文”文本提取出来，导航栏、广告之类都要过滤。</li>
<li><strong>语言过滤</strong>：比如 Fine Web 只保留主要为英文（超过 65% 都是英文）的网页。其他语言就排除。这带来了模型语言能力差异：只学英文文本，模型可能只擅长英文，不太擅长比如中文等其他语言。但大模型也有一定的迁移能力,一种语言里学到的知识也能用另一种语言表达出来</li>
<li><strong>去重</strong>：如果相同或类似的网页重复出现，需进行去重。</li>
<li><strong>敏感信息剔除</strong>：比如检测到社保号、信用卡号、电话等隐私信息，就过滤掉。</li>
</ul>
<p>总之，会有很多这样的清洗工作。最终拿到的，就是一个规模可能几十 TB、基本都是可读文本的<a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/default/train">大型语料库</a>。然后才能进一步进行“喂给模型”的下一步。</p>
<p>比如我们拿 OpenAI 在训练 GPT-3 时公开的<a href="https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_character_count.csv">数据</a>就能看到,按字符算, 中文资料就只有 0.2% 不到的样子</p>
<figure data-type="image" tabindex="1"><img src="https://nhuji.github.io/post-images/1753857618141.png" alt="" loading="lazy"></figure>
<p>然后占比的和权重的话, 我们可以从 GPT-3 的<a href="https://arxiv.org/pdf/2005.14165">论文</a>看到, 我们上面提到的 Common Crawl 是占比最多的</p>
<figure data-type="image" tabindex="2"><img src="https://nhuji.github.io/post-images/1753857636743.png" alt="" loading="lazy"></figure>
<p>按数量和比例, 排名的话权重最高的是 OpenAI 自己专门筛选出来的高质量数据集 WebText , 像维基百科这种知识可信度很高的来源权重也比较大</p>
<h3 id="将文本表示成-token-序列">将文本表示成 token 序列</h3>
<p>接下来是个关键问题：我们如何把这么多文本，喂给一个神经网络？神经网络擅长处理向量形式或有限离散符号序列。文本本身是字符串，对应到计算机内部一般是 UTF-8 字节序列，但直接用单个 bit 或单个 byte 做输入会过于低效。</p>
<figure data-type="image" tabindex="3"><img src="https://nhuji.github.io/post-images/1753857652397.png" alt="" loading="lazy"></figure>
<p>字符在 UTF-8 时的对应就和文本和 token 的对应类似</p>
<p>就像是UTF-8 文字编码的对应, LLM 里会使用一种 “分词” 技术，也叫 <strong>tokenization</strong>，也是将文本对应成一个个 token 。每个 token 代表一个文本单元，以减少序列长度。这些 tokens 可以是单个字符、单词的一部分，甚至是整个单词或句子片段</p>
<p>比较常见的一种方法是 <strong>Byte Pair Encoding (BPE)</strong> 或衍生的方法。它会根据文本统计，自动将常见的子词或字符串组合成一个 token，从而减少序列长度。</p>
<p>现代大模型通常用 5 万到几十万规模的 token 词表（vocabulary）。例如 GPT-4 使用了 100,277 个 token 的词表。然后，给定任意文本，就能被分割成一串 token，每个 token 其实就是一个 ID（数字），模型会把它们作为输入。</p>
<p>例如 “Hello world” 可能会被切分成 2 个 token: <code>[Hello]</code> 和 <code>[ world]</code>；如果加了大写或符号，分词可能会不同。同样一句话在不同模型上也可能产生不同数量的 token, 因为用的分词表可能有差异, 当无法将一个字符作为完整 token 处理时，分词器会将其分解为字节级别的组成部分</p>
<h3 id="文本怎么被分词">文本怎么被分词</h3>
<figure data-type="image" tabindex="4"><img src="https://nhuji.github.io/post-images/1753857675431.png" alt="" loading="lazy"></figure>
<p>比如我们乱写一句中文, 可以看到它被分成多个 tokens 来对应词</p>
<p>那么聊天时的整个上下文,怎么被分成 tokens 可以在 <a href="https://tiktokenizer.vercel.app/">Tiktokenizer</a>  上看到 ⬇️</p>
<figure data-type="image" tabindex="5"><img src="https://nhuji.github.io/post-images/1753857704478.png" alt="" loading="lazy"></figure>
<p>比如 1️⃣ 那里那样的简单的对话, 在处理时约等于 2️⃣ 的文字序列,其中&lt;|im_start|&gt;这样的标签和里面的文字用于区分用户和 AI 的回答和内容</p>
<p>经过分词后,就像 3️⃣ 那里将整个文本序列分成多块,对应 4️⃣ 的数字 tokens 序列, LLM 就会在这个基础上接着预测来处理,然后返回的内容就是 AI 对你的回答了, 也可以看看官方的分词器来看看一句话大概有多少 token <a href="https://platform.openai.com/tokenizer">OpenAI Platform</a></p>
<p>如果好奇数字和文本是怎么对应的,也可以看看下面这个 GPT-4 里 token 和文字的对应的分词表</p>
<p><a href="https://gist.github.com/s-macke/ae83f6afb89794350f8d9a1ad8a09193#file-gpt-4-tokens-txt-L2013">All 100k GPT-4 Tokens</a></p>
<p>简单的总结一下之所以不用文本而是要 token 化 是这样计算效率更高,也能支持更多的语言,加快训练速度,而且不光是文本, 图像和音频也能被 tokenization</p>
<p>这也解释了,为什么 LLM 的数学不太好,数不清楚 9.11 和 9.9 谁大, 或者 Strawberry 里有多少个 R</p>
<p>比如, &quot;9.11&quot;被分词为[&quot;9.&quot;, &quot;11&quot;]，而&quot;9.9&quot;被分词为[&quot;9.&quot;, &quot;9&quot;],</p>
<p>在比较时，这样模型在比较时，容易将其视为字符或字符串的比较，而不是数值大小的比较，</p>
<p>由于11 &gt; 9，它错误地得出9.11 &gt; 9.9的结论,</p>
<p>所以这也是为什么 LLM 不擅长数学计算,</p>
<p>但如果明确要求模型进行逐步推理（chain-of-thought）或调用外部计算工具就能解决</p>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://nhuji.github.io/tag/X65AQq1vh_/" class="tag">
                    LLM
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://nhuji.github.io/post/PbbB545riV/">
                  <h3 class="post-title">
                    Whisk: 不用写提示词的有趣生图工具
                  </h3>
                </a>
              </div>
            
            <!-- 像素图 -->
            
            <div id="doodle">
              <css-doodle  click-to-update >
                 @grid: 16x12 / 800px auto;
                  @size: 6px;
                  box-shadow: @m3x5(
                    calc(18px - @nx(-1) * 6px) calc(@ny * 6px)
                      0 @p(@m3(#000), @m2(transparent)),
                    calc(18px + @nx(-1) * 6px) calc(@ny * 6px)
                      0 @lp
                  );
              </css-doodle>
           
              <button onclick="startAnimation()">点击这里变得狂野</button>
            </div>

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>






   
    <script src="https://unpkg.com/css-doodle@0.34.1/css-doodle.min.js"></script>
     <!-- 用于判断css doodle的显示 -->
    <script>
      var doodle = document.getElementById("doodle");
      var pageTitle = document.title;
      if (pageTitle === "关于 | Huhu's blog") {
        doodle.style.display = 'block';
      } else {
        doodle.style.display = 'none';
      }
    </script>



    <!-- doodle更新 -->
    <script>
      const cssDoodle = document.querySelector('css-doodle');

      // 定义时间间隔变量，以毫秒为单位
      let interval = 2000;

      // 自动更新(播放)CSS-Doodle样式
      function animateDoodle() {
        cssDoodle.update();
        // 在10毫秒后再次调用此函数
        setTimeout(animateDoodle, interval);
      }
      
      function startAnimation() {
        interval /= 1.5;
        cssDoodle.update(`
         @grid: 16x12 / 800px auto;
         @size: 6px;
         color: hsl(@r240, 30%, 50%);
         box-shadow: @m3x5(
          calc(18px - @nx(-1) * 6px) calc(@ny * 6px)
            0 @p(@m3(currentColor), @m2(transparent)),
          calc(18px + @nx(-1) * 6px) calc(@ny * 6px)
            0 @lp
          );  
      `);
        // 开始快速更新动画
        animateDoodle();
      }


     
    </script>
  
  </body>
</html>
